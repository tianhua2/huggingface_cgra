# ุงูุชูููุฏ ุจุงุณุชุฎุฏุงู LLMs

[[open-in-colab]]

ุชุนุฏ LLMsุ ุฃู ููุงุฐุฌ ุงููุบุฉ ุงููุจูุฑุฉุ ุงููููู ุงูุฑุฆูุณู ูุฑุงุก ุชูููุฏ ุงููุต. ูุจุงุฎุชุตุงุฑุ ุชุชููู ูู ููุงุฐุฌ ูุญูู ูุจูุฑุฉ ูุณุจูุฉ ุงูุชุฏุฑูุจ ุชู ุชุฏุฑูุจูุง ููุชูุจุค ุจุงููููุฉ ุงูุชุงููุฉ (ุฃูุ ุจุดูู ุฃูุซุฑ ุฏูุฉุ ุงูุฑูุฒ) ุจุงููุธุฑ ุฅูู ุจุนุถ ุงููุต ุงููุฏุฎู. ูุธุฑูุง ูุฃููุง ุชุชูุจุฃ ุจุฑูุฒ ูุงุญุฏ ูู ูู ูุฑุฉุ ูุฌุจ ุนููู ุงูููุงู ุจุดูุก ุฃูุซุฑ ุชูุตููุงู ูุชูููุฏ ุฌูู ุฌุฏูุฏุฉ ุจุฎูุงู ูุฌุฑุฏ ุงุณุชุฏุนุงุก ุงููููุฐุฌ - ูุฌุจ ุนููู ุฅุฌุฑุงุก ุงูุชูููุฏ ุงูุชููุงุฆู.

ุงูุชูููุฏ ุงูุชููุงุฆู ูู ุฅุฌุฑุงุก ููุช ุงูุงุณุชุฏูุงู ุงูุฐู ูุณุชุฏุนู ุงููููุฐุฌ ุจุดูู ุชูุฑุงุฑู ูุน ุงูุฅุฎุฑุงุฌ ุงูุฐู ุชู ุฅูุดุงุคูุ ุจุงููุธุฑ ุฅูู ุจุนุถ ุงูุฅุฏุฎุงูุงุช ุงูุฃูููุฉ. ูู ๐ค Transformersุ ูุชู ุงูุชุนุงูู ูุน ูุฐุง ุจูุงุณุทุฉ ุทุฑููุฉ [`~generation.GenerationMixin.generate`]ุ ูุงูุชู ุชุชููุฑ ูุฌููุน ุงูููุงุฐุฌ ุฐุงุช ุงููุฏุฑุงุช ุงูุชูููุฏูุฉ.

ุณููุถุญ ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนูููู ููููุฉ:

* ุชูููุฏ ุงููุต ุจุงุณุชุฎุฏุงู LLM
* ุชุฌูุจ ุงููููุน ูู ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ
* ุงูุฎุทูุงุช ุงูุชุงููุฉ ููุณุงุนุฏุชู ูู ุงูุงุณุชูุงุฏุฉ ุงููุตูู ูู LLM ุงูุฎุงุต ุจู

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:

```bash
pip install transformers bitsandbytes>=0.39.0 -q
```

## ุชูููุฏ ุงููุต

ูุฃุฎุฐ ูููุฐุฌ ุงููุบุฉ ุงููุฏุฑุจ ูู [ููุฐุฌุฉ ุงููุบุฉ ุงูุณุจุจูุฉ](tasks/language_modeling) ุชุณูุณู ุฑููุฒ ุงููุต ูุฅุฏุฎุงู ููุนูุฏ ุชูุฒูุน ุงูุงุญุชูุงููุฉ ููุฑูุฒ ุงูุชุงูู.

<!-- [GIF 1 -- FWD PASS] -->
<figure class="image table text-center m-0 w-full">
    <video
        style="max-width: 90%; margin: auto;"
        autoplay loop muted playsinline
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov"
    ></video>
    <figcaption>"Forward pass of an LLM"</figcaption>
</figure>

ููุงู ุฌุงูุจ ุจุงูุบ ุงูุฃูููุฉ ูู ุงูุชูููุฏ ุงูุชููุงุฆู ุจุงุณุชุฎุฏุงู LLMs ููู ููููุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒ ุงูุชุงูู ูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ูุฐุง. ูู ุดูุก ูุณููุญ ุจู ูู ูุฐู ุงูุฎุทูุฉ ุทุงููุง ุฃูู ุชูุชูู ุจุฑูุฒ ููุชูุฑุงุฑ ุงูุชุงูู. ููุฐุง ูุนูู ุฃูู ูููู ุฃู ูููู ุจุณูุทูุง ูุซู ุงุฎุชูุงุฑ ุงูุฑูุฒ ุงูุฃูุซุฑ ุงุญุชูุงููุง ูู ุชูุฒูุน ุงูุงุญุชูุงููุฉ ุฃู ูุนูุฏูุง ูุซู ุชุทุจูู ุนุดุฑุงุช ุงูุชุญููุงุช ูุจู ุฃุฎุฐ ุงูุนููุงุช ูู ุงูุชูุฒูุน ุงููุงุชุฌ.

<!-- [GIF 2 -- TEXT GENERATION] -->
<figure class="image table text-center m-0 w-full">
    <video
        style="max-width: 90%; margin: auto;"
        autoplay loop muted playsinline
        src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov"
    ></video>
    <figcaption>"Autoregressive generation iteratively selects the next token from a probability distribution to generate text"</figcaption>
</figure>

ุชุชูุฑุฑ ุงูุนูููุฉ ุงูููุถุญุฉ ุฃุนูุงู ุจุดูู ุชูุฑุงุฑู ุญุชู ูุชู ุงููุตูู ุฅูู ุดุฑุท ุงูุชููู. ูู ุงููุถุน ุงููุซุงููุ ูุญุฏุฏ ุงููููุฐุฌ ุดุฑุท ุงูุชูููุ ูุงูุฐู ูุฌุจ ุฃู ูุชุนูู ุนูุฏ ุฅุฎุฑุงุฌ ุฑูุฒ ููุงูุฉ ุงูุชุณูุณู (`EOS`). ุฅุฐุง ูู ููู ุงูุฃูุฑ ูุฐููุ ูุชููู ุงูุชูููุฏ ุนูุฏ ุงููุตูู ุฅูู ุทูู ุฃูุตู ูุญุฏุฏ ูุณุจููุง.

ูู ุงูุถุฑูุฑู ุฅุนุฏุงุฏ ุฎุทูุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒ ูุดุฑุท ุงูุชููู ุจุดูู ุตุญูุญ ูุฌุนู ูููุฐุฌู ูุชุตุฑู ููุง ุชุชููุน ูู ูููุชู. ูููุฐุง ุงูุณุจุจ ูุฏููุง [`~generation.GenerationConfig`] ููู ูุฑุชุจุท ุจูู ูููุฐุฌุ ูุงูุฐู ูุญุชูู ุนูู ูุนููุฉ ุชูููุฏูุฉ ุงูุชุฑุงุถูุฉ ุฌูุฏุฉ ููุชู ุชุญูููู ุฌูุจูุง ุฅูู ุฌูุจ ูุน ูููุฐุฌู.

ุฏุนูุง ูุชุญุฏุซ ุนู ุงูููุฏ!

<Tip>
ุฏุนูุง ูุชุญุฏุซ ุนู ุงูููุฏ!

<Tip>

ุฅุฐุง ููุช ููุชููุง ุจุงูุงุณุชุฎุฏุงู ุงูุฃุณุงุณู ูู LLMุ ูุฅู ูุงุฌูุฉ [`Pipeline`](pipeline_tutorial) ุนุงููุฉ ุงููุณุชูู ูุฏููุง ูู ููุทุฉ ุงูุทูุงู ุฑุงุฆุนุฉ. ููุน ุฐููุ ุบุงูุจูุง ูุง ุชุชุทูุจ LLMs ููุฒุงุช ูุชูุฏูุฉ ูุซู ุงูุชูููู ูุงูุชุญูู ุงูุฏููู ูู ุฎุทูุฉ ุงุฎุชูุงุฑ ุงูุฑูุฒุ ูุงูุชู ูุชู ุชูููุฐูุง ุจุดูู ุฃูุถู ูู ุฎูุงู [`~generation.GenerationMixin.generate`]. ุงูุชูููุฏ ุงูุชููุงุฆู ุจุงุณุชุฎุฏุงู LLMs ูุซูู ุงูุงุณุชุฎุฏุงู ููููุงุฑุฏ ููุฌุจ ุชูููุฐู ุนูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช ููุญุตูู ุนูู ุงูุฅูุชุงุฌูุฉ ุงููุงููุฉ.

</Tip>

ุฃููุงูุ ุชุญุชุงุฌ ุฅูู ุชุญููู ุงููููุฐุฌ.

```py
>>> from transformers import AutoModelForCausalLM

>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

ุณุชูุงุญุธ ูุฌูุฏ ุนูููู ูู ููุงููุฉ `from_pretrained`:

 - `device_map` ูุถูู ุงูุชูุงู ุงููููุฐุฌ ุฅูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ุงูุฎุงุตุฉ ุจู
 - `load_in_4bit` ูุทุจู [4-bit dynamic quantization](main_classes/quantization) ูุฎูุถ ูุชุทูุจุงุช ุงูููุงุฑุฏ ุจุดูู ูุจูุฑ

ููุงู ุทุฑู ุฃุฎุฑู ูุชููุฆุฉ ูููุฐุฌุ ูููู ูุฐุง ุฎุท ุฃุณุงุณ ุฌูุฏ ููุจุฏุก ุจุงุณุชุฎุฏุงู LLM.

ุจุนุฏ ุฐููุ ุชุญุชุงุฌ ุฅูู ูุนุงูุฌุฉ ุฅุฏุฎุงู ุงููุต ุงูุฎุงุต ุจู ุจุงุณุชุฎุฏุงู [ูุตูู ุงูุฑููุฒ](tokenizer_summary).

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> model_inputs = tokenizer(["A list of colors: red, blue"], return_tensors="pt").to("cuda")
```

ูุญุชูู ูุชุบูุฑ `model_inputs` ุนูู ุฅุฏุฎุงู ุงููุต ุงููุนุงูุฌุ ุจุงูุฅุถุงูุฉ ุฅูู ููุงุน ุงูุงูุชูุงู. ูู ุญูู ุฃู [`~generation.GenerationMixin.generate`] ุชุจุฐู ูุตุงุฑู ุฌูุฏูุง ูุงุณุชูุชุงุฌ ููุงุน ุงูุงูุชูุงู ุนูุฏูุง ูุง ูุชู ุชูุฑูุฑูุ ููุตู ุจุชูุฑูุฑู ูููุง ุฃููู ุฐูู ููุญุตูู ุนูู ูุชุงุฆุฌ ูุซุงููุฉ.

ุจุนุฏ ุชูููู ุงููุฏุฎูุงุชุ ููููู ุงุณุชุฏุนุงุก ุทุฑููุฉ [`~generation.GenerationMixin.generate`] ูุฅุฑุฌุงุน ุงูุฑููุฒ ุงููููุฏุฉ. ูุฌุจ ุจุนุฏ ุฐูู ุชุญููู ุงูุฑููุฒ ุงููููุฏุฉ ุฅูู ูุต ูุจู ุงูุทุจุงุนุฉ.

```py
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A list of colors: red, blue, green, yellow, orange, purple, pink,'
```

ุฃุฎูุฑูุงุ ูุง ุชุญุชุงุฌ ุฅูู ุงูููุงู ุจุฐูู ุชุณูุณู ูุงุญุฏ ูู ูู ูุฑุฉ! ููููู ุฏูุฌ ุฅุฏุฎุงูุงุชูุ ูุงูุชู ุณุชุนูู ุนูู ุชุญุณูู ุงูุฅูุชุงุฌูุฉ ุจุดูู ูุจูุฑ ุจุชูููุฉ ุตุบูุฑุฉ ูู ุงููููู ูุงูุฐุงูุฑุฉ. ูู ูุง ุนููู ุงูุชุฃูุฏ ููู ูู ุฃูู ุชููู ุจุชูุณูุท ุฅุฏุฎุงูุงุชู ุจุดูู ุตุญูุญ (ุงููุฒูุฏ ุญูู ุฐูู ุฃุฏูุงู).

```py
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["A list of colors: red, blue", "Portugal is"], return_tensors="pt", padding=True
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
['A list of colors: red, blue, green, yellow, orange, purple, pink,',
'Portugal is a country in southwestern Europe, on the Iber']
```

ููุฐุง ูู ุดูุก! ูู ุจุถุน ุณุทูุฑ ูู ุงูุชุนูููุงุช ุงูุจุฑูุฌูุฉุ ููููู ุชุณุฎูุฑ ููุฉ LLM.

## ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ

ููุงู ุงูุนุฏูุฏ ูู [ุงุณุชุฑุงุชูุฌูุงุช ุงูุชูููุฏ](generation_strategies)ุ ููู ุจุนุถ ุงูุฃุญูุงู ูุฏ ูุง ุชููู ุงูููู ุงูุงูุชุฑุงุถูุฉ ููุงุณุจุฉ ูุญุงูุชู ุงูุงุณุชุฎุฏุงู. ุฅุฐุง ูู ุชูู ุงูุฅุฎุฑุงุฌ ุงูุฎุงุตุฉ ุจู ูุชูุงููุฉ ูุน ูุง ุชุชููุนูุ ููุฏ ูููุง ุจุฅูุดุงุก ูุงุฆูุฉ ุจุฃูุซุฑ ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ ูููููุฉ ุชุฌูุจูุง.

```py
>>> from transformers import AutoModelForCausalLM, AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model = AutoModelForCausalLM.from_pretrained(
...     "mistralai/Mistral-7B-v0.1", device_map="auto", load_in_4bit=True
... )
```

### ุงูุฅุฎุฑุงุฌ ุงููููุฏ ูุตูุฑ ุฌุฏูุง/ุทููู ุฌุฏูุง

ุฅุฐุง ูู ูุชู ุชุญุฏูุฏู ูู [`~generation.GenerationConfig`] ุงููููุ `generate` ูุนูุฏ ูุง ูุตู ุฅูู 20 ุฑูุฒูุง ุจุดูู ุงูุชุฑุงุถู. ููุตู ุจุดุฏุฉ ุจุชุนููู `max_new_tokens` ูุฏูููุง ูู ููุงููุฉ `generate` ููุชุญูู ูู ุงูุนุฏุฏ ุงูุฃูุตู ูู ุงูุฑููุฒ ุงูุฌุฏูุฏุฉ ุงูุชู ูููู ุฃู ูุนูุฏูุง. ุถุน ูู ุงุนุชุจุงุฑู ุฃู LLMs (ุจุดูู ุฃูุซุฑ ุฏูุฉุ [ููุงุฐุฌ ูู ุงูุชุดููุฑ ููุท](https://huggingface.co/learn/nlp-course/chapter1/6ุfw=pt)) ุชุนูุฏ ุฃูุถูุง ููุฌู ุงูุฅุฏุฎุงู ูุฌุฒุก ูู ุงูุฅุฎุฑุงุฌ.
```py
>>> model_inputs = tokenizer(["A sequence of numbers: 1, 2"], return_tensors="pt").to("cuda")

>>> # By default, the output will contain up to 20 tokens
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5'

>>> # Setting `max_new_tokens` allows you to control the maximum length
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=50)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,'
```

### ูุถุน ุงูุชูููุฏ ุบูุฑ ุงูุตุญูุญ

ุจุดูู ุงูุชุฑุงุถูุ ููุง ูู ูุชู ุชุญุฏูุฏู ูู [`~generation.GenerationConfig`] ุงููููุ `generate` ูุญุฏุฏ ุงูุฑูุฒ ุงูุฃูุซุฑ ุงุญุชูุงููุง ูู ูู ุชูุฑุงุฑ (ูู ุชุดููุฑ ุฌุดุน). ุงุนุชูุงุฏูุง ุนูู ูููุชูุ ูุฏ ูููู ูุฐุง ุบูุฑ ูุฑุบูุจ ูููุ ุชุณุชููุฏ ุงูููุงู ุงูุฅุจุฏุงุนูุฉ ูุซู ุจุฑุงูุฌ ุงูุฏุฑุฏุดุฉ ุฃู ูุชุงุจุฉ ููุงู ูู ุฃุฎุฐ ุงูุนููุงุช. ูู ูุงุญูุฉ ุฃุฎุฑูุ ุชุณุชููุฏ ุงูููุงู ุงููุณุชูุฏุฉ ุฅูู ุงูุฅุฏุฎุงู ูุซู ูุณุฎ ุงููุต ุงูุตูุชู ุฃู ุงูุชุฑุฌูุฉ ูู ูู ุงูุชุดููุฑ ุงูุฌุดุน. ูู ุจุชูููู ุงูุนููุงุช ุจุงุณุชุฎุฏุงู `do_sample=True`ุ ูููููู ูุนุฑูุฉ ุงููุฒูุฏ ุญูู ูุฐุง ุงูููุถูุน ูู [ุชุฏูููุฉ ุงููุฏููุฉ](https://huggingface.co/blog/how-to-generate).

```py
>>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility
>>> from transformers import set_seed
>>> set_seed(42)

>>> model_inputs = tokenizer(["I am a cat."], return_tensors="pt").to("cuda")

>>> # LLM + greedy decoding = repetitive, boring output
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat. I am a cat. I am a cat. I am a cat'

>>> # With sampling, the output becomes more creative!
>>> generated_ids = model.generate(**model_inputs, do_sample=True)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'I am a cat.  Specifically, I am an indoor-only cat.  I'
```

### ุฌุงูุจ ุงูุชูุณูุท ุบูุฑ ุงูุตุญูุญ

LLMs ูู [ูุนูุงุฑูุงุช ูู ุงูุชุดููุฑ ููุท](https://huggingface.co/learn/nlp-course/chapter1/6ุfw=pt)ุ ููุง ูุนูู ุฃููุง ุชุณุชูุฑ ูู ุงูุชูุฑุงุฑ ุนูู ููุฌู ุงูุฅุฏุฎุงู ุงูุฎุงุต ุจู. ุฅุฐุง ูู ููู ูุฅุฏุฎุงูุงุชู ููุณ ุงูุทููุ ููุฌุจ ุชูุณูุทูุง. ูุธุฑูุง ูุฃู LLMs ุบูุฑ ูุฏุฑุจุฉ ููุงุณุชูุฑุงุฑ ูู ุฑููุฒ ุงูุชูุณูุทุ ูุฌุจ ุชูุณูุท ุงูุฅุฏุฎุงู ุงูุฎุงุต ุจู ูู ุงููุณุงุฑ. ุชุฃูุฏ ูู ุนุฏู ูุณูุงู ุชูุฑูุฑ ููุงุน ุงูุงูุชูุงู ุฅูู ุงูุชูููุฏ!

```py
>>> # The tokenizer initialized above has right-padding active by default: the 1st sequence,
>>> # which is shorter, has padding on the right side. Generation fails to capture the logic.
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 33333333333'

>>> # With left-padding, it works as expected!
>>> tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1", padding_side="left")
>>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default
>>> model_inputs = tokenizer(
...     ["1, 2, 3", "A, B, C, D, E"], padding=True, return_tensors="pt"
... ).to("cuda")
>>> generated_ids = model.generate(**model_inputs)
>>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
'1, 2, 3, 4, 5, 6,'
```

### ููุฌู ุบูุฑ ุตุญูุญ

ุชุชููุน ุจุนุถ ุงูููุงุฐุฌ ูุงูููุงู ุชูุณูู ููุฌู ุงูุฅุฏุฎุงู ุงููุญุฏุฏ ููุนูู ุจุดูู ุตุญูุญ. ุนูุฏูุง ูุง ูุชู ุชุทุจูู ูุฐุง ุงูุชูุณููุ ูุณุชุญุตู ุนูู ุชุฏููุฑ ุตุงูุช ูู ุงูุฃุฏุงุก: ูุนูู ุงููููุฐุฌ ุจุดูู ุฌูุฏุ ููููู ููุณ ุฌูุฏูุง ููุง ูู ููุช ุชุชุจุน ุงูููุฌู ุงููุชููุน. ุชุชููุฑ ูุนูููุงุช ุฅุถุงููุฉ ุญูู ุงูุชูุฌููุ ุจูุง ูู ุฐูู ุงูููุงุฐุฌ ูุงูููุงู ุงูุชู ุชุญุชุงุฌ ุฅูู ุชูุฎู ุงูุญุฐุฑุ ูู [ุงูุฏููู](tasks/prompting). ุฏุนูุง ูุฑู ูุซุงูุงู ุจุงุณุชุฎุฏุงู LLM ููุฏุฑุฏุดุฉุ ูุงูุฐู ูุณุชุฎุฏู [ูุงูุจ ุงูุฏุฑุฏุดุฉ](chat_templating):
```python
>>> tokenizer = AutoTokenizer.from_pretrained("HuggingFaceH4/zephyr-7b-alpha")
>>> model = AutoModelForCausalLM.from_pretrained(
...     "HuggingFaceH4/zephyr-7b-alpha", device_map="auto", load_in_4bit=True
... )
>>> set_seed(0)
>>> prompt = """How many helicopters can a human eat in one sitting? Reply as a thug."""
>>> model_inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
>>> input_length = model_inputs.input_ids.shape[1]
>>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
"I'm not a thug, but i can tell you that a human cannot eat"
>>> # Oh no, it did not follow our instruction to reply as a thug! Let's see what happens when we write
>>> # a better prompt and use the right template for this model (through `tokenizer.apply_chat_template`)

>>> set_seed(0)
>>> messages = [
...     {
...         "role": "system",
...         "content": "You are a friendly chatbot who always responds in the style of a thug",
...     },
...     {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
... ]
>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to("cuda")
>>> input_length = model_inputs.shape[1]
>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)
>>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])
'None, you thug. How bout you try to focus on more useful questions?'
>>> # As we can see, it followed a proper thug style ๐
```

## ููุงุฑุฏ ุฅุถุงููุฉ

ูู ุญูู ุฃู ุนูููุฉ ุงูุชูููุฏ ุงูุชููุงุฆู ุจุณูุทุฉ ูุณุจููุงุ ูุฅู ุงูุงุณุชูุงุฏุฉ ุงููุตูู ูู LLM ุงูุฎุงุต ุจู ูููู ุฃู ุชููู ูููุฉ ุตุนุจุฉ ูุฃู ููุงู ุงูุนุฏูุฏ ูู ุงูุฃุฌุฒุงุก ุงููุชุญุฑูุฉ. ููุฎุทูุงุช ุงูุชุงููุฉ ููุณุงุนุฏุชู ูู ุงูุบูุต ุจุดูู ุฃุนูู ูู ุงุณุชุฎุฏุงู LLM ููููู:

### ุงุณุชุฎุฏุงู ูุชูุฏู ููุชูููุฏ

1. ุฏููู ุญูู ููููุฉ [ุงูุชุญูู ูู ุทุฑู ุงูุชูููุฏ ุงููุฎุชููุฉ](generation_strategies)ุ ูููููุฉ ุฅุนุฏุงุฏ ููู ุชูููู ุงูุชูููุฏุ ูููููุฉ ุจุซ ุงูุฅุฎุฑุงุฌุ
2. [ุชุณุฑูุน ุชูููุฏ ุงููุต](llm_optims)ุ
3. [ููุงูุจ ููุฌูุงุช ููุฏุฑุฏุดุฉ LLMs](chat_