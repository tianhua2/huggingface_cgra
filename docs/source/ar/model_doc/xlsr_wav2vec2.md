# XLSR-Wav2Vec2

## نظرة عامة

اقترح نموذج XLSR-Wav2Vec2 في [تعلم تمثيل الكلام متعدد اللغات دون إشراف للتعرف على الكلام](https://arxiv.org/abs/2006.13979) بواسطة Alexis Conneau، Alexei Baevski، Ronan Collobert، Abdelrahman Mohamed، Michael
Auli.

الملخص من الورقة هو ما يلي:

*تقدم هذه الورقة XLSR الذي يتعلم تمثيلات الكلام متعددة اللغات عن طريق التدريب المسبق لنموذج واحد من الموجة الحاملة الخام للكلام في عدة لغات. نحن نبني على wav2vec 2.0 الذي يتم تدريبه عن طريق حل مهمة متناقضة على 
تمثيلات الكلام الكامنة المقنعة وتتعلم بشكل مشترك تشفير الكامنات المشتركة عبر اللغات.
تم ضبط النموذج الناتج باستخدام بيانات ذات علامات وتُظهر التجارب أن التدريب المسبق متعدد اللغات يتفوق بشكل كبير على التدريب المسبق أحادي اللغة. على معيار CommonVoice، يظهر XLSR انخفاضًا نسبيًا في معدل أخطاء فونيم بنسبة 72% مقارنة بأفضل النتائج المعروفة. على BABEL، يحسن نهجنا معدل خطأ الكلمة بنسبة 16% مقارنة بنظام مماثل. يمكّن نهجنا نموذجا واحدا للتعرف على الكلام متعدد اللغات يكون منافسا للنماذج الفردية القوية. يُظهر التحليل أن تمثيلات الكلام الكامنة المنفصلة مشتركة عبر اللغات مع زيادة المشاركة للغات ذات الصلة. نأمل أن نحفز البحث في فهم الكلام منخفض الموارد عن طريق إصدار XLSR-53، وهو نموذج كبير تم تدريبه مسبقًا على 53 لغة.*

يمكن العثور على الكود الأصلي [هنا](https://github.com/pytorch/fairseq/tree/master/fairseq/models/wav2vec).

ملاحظة: أصدرت Meta (FAIR) إصدارًا جديدًا من [Wav2Vec2-BERT 2.0](https://huggingface.co/docs/transformers/en/model_doc/wav2vec2-bert) - فهو مُدرب مسبقًا على 4.5 مليون ساعة من الصوت. نوصي بشكل خاص باستخدامه لمهام الضبط الدقيق، على سبيل المثال كما هو موضح في [هذا الدليل](https://huggingface.co/blog/fine-tune-w2v2-bert).

## نصائح الاستخدام

- XLSR-Wav2Vec2 هو نموذج كلام يقبل صفيفًا عائمًا يتوافق مع الموجة الحاملة الخام لإشارة الكلام.
- تم تدريب نموذج XLSR-Wav2Vec2 باستخدام التصنيف الزمني للاتصال (CTC)، لذلك يجب فك تشفير إخراج النموذج
  باستخدام [`Wav2Vec2CTCTokenizer`].

<Tip>

تستند بنية XLSR-Wav2Vec2 إلى نموذج Wav2Vec2، لذلك يمكن الرجوع إلى [صفحة وثائق Wav2Vec2](wav2vec2).

</Tip
