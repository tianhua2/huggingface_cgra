# ุชุณุฑูุน ุงูุงุณุชูุชุงุฌ ุจุงุณุชุฎุฏุงู torch.compile()

ููุฏู ูุฐุง ุงูุฏููู ุฅูู ุชูุฏูู ูุนูุงุฑ ูููุงุณ ุงูุชุญุณููุงุช ูู ุณุฑุนุฉ ุงูุงุณุชูุชุงุฌ ุงูุชู ุชู ุชูุฏูููุง ูุน [torch.compile()](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) [ูููุงุฐุฌ ุงูุฑุคูุฉ ุงูุญุงุณูุจูุฉ ูู ๐ค Transformers](https://huggingface.co/modelsุpipeline_tag=image-classification&library=transformers&sort=trending).

## ููุงุฆุฏ torch.compile

ุจูุงุกู ุนูู ุงููููุฐุฌ ูุจุทุงูุฉ GPUุ ูููุฑ torch.compile() ุชุณุฑูุนูุง ูุตู ุฅูู 30% ุฃุซูุงุก ุงูุงุณุชูุชุงุฌ. ูุงุณุชุฎุฏุงู torch.compile()ุ ูุง ุนููู ุณูู ุชุซุจูุช ุฃู ุฅุตุฏุงุฑ ูู torch ุฃุนูู ูู 2.0.

ูุณุชุบุฑู ุชุฌููุน ูููุฐุฌ ููุชูุงุ ูุฐุง ููู ูููุฏ ุฅุฐุง ููุช ุชููู ุจุชุฌููุน ุงููููุฐุฌ ูุฑุฉ ูุงุญุฏุฉ ููุท ุจุฏูุงู ูู ูู ูุฑุฉ ุชููู ูููุง ุจุงูุงุณุชูุชุงุฌ. ูุชุฌููุน ุฃู ูููุฐุฌ ุฑุคูุฉ ุญุงุณูุจูุฉ ูู ุงุฎุชูุงุฑูุ ูู ุจุงูุงุณุชุฏุนุงุก `torch.compile()` ุนูู ุงููููุฐุฌ ููุง ูู ููุถุญ ุฃุฏูุงู:

```diff
from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to("cuda")
+ model = torch.compile(model)
```

ูุฃุชู `compile()` ูุน ุฃูุถุงุน ูุชุนุฏุฏุฉ ููุชุฌููุนุ ูุงูุชู ุชุฎุชูู ุจุดูู ุฃุณุงุณู ูู ููุช ุงูุชุฌููุน ูุนุจุก ุงูุงุณุชุฏูุงู. ูุณุชุบุฑู ุงูุฃูุฑ `max-autotune` ููุชูุง ุฃุทูู ูู `reduce-overhead` ููููู ูุคุฏู ุฅูู ุงุณุชุฏูุงู ุฃุณุฑุน. ุงููุถุน ุงูุงูุชุฑุงุถู ูู ุงูุฃุณุฑุน ููุชุฌููุน ููููู ููุณ ุจููุงุกุฉ `reduce-overhead` ูููุช ุงูุงุณุชุฏูุงู. ูู ูุฐุง ุงูุฏูููุ ุงุณุชุฎุฏููุง ุงููุถุน ุงูุงูุชุฑุงุถู. ููููู ูุนุฑูุฉ ุงููุฒูุฏ ุนูู [ููุง](https://pytorch.org/get-started/pytorch-2.0/#user-experience).

ูููุง ุจุงุฎุชุจุงุฑ `torch.compile` ูุน ููุงุฐุฌ ุฑุคูุฉ ุญุงุณูุจูุฉ ูุฎุชููุฉุ ููููุงุชุ ูุฃููุงุน ุงูุฃุฌูุฒุฉุ ูุฃุญุฌุงู ุงูุฏูุนุงุช ุนูู ุฅุตุฏุงุฑ `torch` 2.0.1.

## ููุฏ ุงููุนูุงุฑ ุงููุฑุฌุนู

ูููุง ููู ููููู ุงูุนุซูุฑ ุนูู ููุฏ ุงููุนูุงุฑ ุงููุฑุฌุนู ููู ูููุฉ. ูููู ุจุชุณุฎูู ูุญุฏุฉ ูุนุงูุฌุฉ ุงูุฑุณููุงุช (GPU) ูุจู ุงูุงุณุชุฏูุงู ููุฃุฎุฐ ูุชูุณุท ููุช 300 ุงุณุชุฏูุงูุ ุจุงุณุชุฎุฏุงู ููุณ ุงูุตูุฑุฉ ูู ูู ูุฑุฉ.

### ุชุตููู ุงูุตูุฑ ูุน ViT

```python
import torch
from PIL import Image
import requests
import numpy as np
from transformers import AutoImageProcessor, AutoModelForImageClassification

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)

processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224")
model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224").to("cuda")
model = torch.compile(model)

processed_input = processor(image, return_tensors='pt').to(device="cuda")

with torch.no_grad():
    _ = model(**processed_input)
```

#### ุงูุชุดุงู ุงููุงุฆูุงุช ูุน DETR

```python
from transformers import AutoImageProcessor, AutoModelForObjectDetection

processor = AutoImageProcessor.from_pretrained("facebook/detr-resnet-50")
model = AutoModelForObjectDetection.from_pretrained("facebook/detr-resnet-50").to("cuda")
model = torch.compile(model)

texts = ["a photo of a cat", "a photo of a dog"]
inputs = processor(text=texts, images=image, return_tensors="pt").to("cuda")

with torch.no_grad():
    _ = model(**inputs)
```

#### ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ ูุน Segformer

```python
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation
with torch.no_grad():
    _ = model(**inputs)
```

#### ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ ูุน Segformer

```python
from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation

processor = SegformerImageProcessor.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512")
model = SegformerForSemanticSegmentation.from_pretrained("nvidia/segformer-b0-finetuned-ade-512-512").to("cuda")
model = torch.compile(model)
seg_inputs = processor(images=image, return_tensors="pt").to("cuda")

with torch.no_grad():
    _ = model(**seg_inputs)
```

ูููุง ููู ูุงุฆูุฉ ุจุงูููุงุฐุฌ ุงูุชู ุฃุฌุฑููุง ุนูููุง ุงููุนูุงุฑ ุงููุฑุฌุนู.

**ุชุตููู ุงูุตูุฑ**
- [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)
- [microsoft/beit-base-patch16-224-pt22k-ft22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)
- [facebook/convnext-large-224](https://huggingface.co/facebook/convnext-large-224)
- [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50)

**ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ**
- [nvidia/segformer-b0-finetuned-ade-512-512](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)
- [facebook/mask2former-swin-tiny-coco-panoptic](https://huggingface.co/facebook/mask2former-swin-tiny-coco-panoptic)
- [facebook/maskformer-swin-base-ade](https://huggingface.co/facebook/maskformer-swin-base-ade)
- [google/deeplabv3_mobilenet_v2_1.0_513](https://huggingface.co/google/deeplabv3_mobilenet_v2_1.0_513)

**ุงูุชุดุงู ุงููุงุฆูุงุช**
- [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)
- [facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)
- [microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)

ูููุง ููู ููููู ุงูุนุซูุฑ ุนูู ุฑุณููุงุช ุจูุงููุฉ ููุฏุฏ ุงูุงุณุชุฏูุงู ูุน ูุจุฏูู `torch.compile()` ูุงููุณุจ ุงููุฆููุฉ ููุชุญุณูู ููู ูููุฐุฌ ูู ุฃุฌูุฒุฉ ูุฃุญุฌุงู ุฏูุนุงุช ูุฎุชููุฉ.

<div class="flex">
  <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/a100_batch_comp.png" />
  </div>
  <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_batch_comp.png" />
  </div>
   <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/t4_batch_comp.png" />
  </div>
</div>

<div class="flex">
  <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_duration.png" />
  </div>
  <div>
    <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_percentage.png" />
  </div>
</div>


![ูุฏุฉ ุงูููุงุฑูุฉ ุนูู V100 ุจุญุฌู ุฏูุนุฉ 1](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_1_duration.png)

![ุงููุณุจุฉ ุงููุฆููุฉ ููุชุญุณูู ุนูู T4 ุจุญุฌู ุฏูุนุฉ 4](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/T4_4_percentage.png)

ูููุง ููู ููููู ุงูุนุซูุฑ ุนูู ูุฏุฏ ุงูุงุณุชุฏูุงู ุจุงููููู ุซุงููุฉ ููู ูููุฐุฌ ูุน ูุจุฏูู `compile()`. ูุงุญุธ ุฃู OwlViT ูุคุฏู ุฅูู OOM ูู ุฃุญุฌุงู ุงูุฏูุนุงุช ุงูุฃูุจุฑ.

### A100 (ุญุฌู ุงูุฏูุนุฉ: 1)

| ุงููููุฉ/ุงููููุฐุฌ | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุจุฏูู ุชุฌููุน | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุชุฌููุน |
|:---:|:---:|:---:|
| ุชุตููู ุงูุตูุฑ/ViT | 9.325 | 7.584 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Segformer | 11.759 | 10.500 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/OwlViT | 24.978 | 18.420 |
| ุชุตููู ุงูุตูุฑ/BeiT | 11.282 | 8.448 | 
| ุงูุชุดุงู ุงููุงุฆูุงุช/DETR | 34.619 | 19.040 |
| ุชุตููู ุงูุตูุฑ/ConvNeXT | 10.410 | 10.208 | 
| ุชุตููู ุงูุตูุฑ/ResNet | 6.531 | 4.124 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Mask2former | 60.188 | 49.117 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Maskformer | 75.764 | 59.487 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/MobileNet | 8.583 | 3.974 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Resnet-101 | 36.276 | 18.197 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Conditional-DETR | 31.219 | 17.993 |
### A100 (ุญุฌู ุงูุฏูุนุฉ: 4)

| ุงููููุฉ/ุงููููุฐุฌ | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุจุฏูู ุชุฌููุน | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุชุฌููุน |
|:---:|:---:|:---:|
| ุชุตููู ุงูุตูุฑ/ViT | 14.832 | 14.499 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Segformer | 18.838 | 16.476 |
| ุชุตููู ุงูุตูุฑ/BeiT | 13.205 | 13.048 | 
| ุงูุชุดุงู ุงููุงุฆูุงุช/DETR | 48.657 | 32.418|
| ุชุตููู ุงูุตูุฑ/ConvNeXT | 22.940 | 21.631 | 
| ุชุตููู ุงูุตูุฑ/ResNet | 6.657 | 4.268 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Mask2former | 74.277 | 61.781 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Maskformer | 180.700 | 159.116 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/MobileNet | 14.174 | 8.515 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Resnet-101 | 68.101 | 44.998 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Conditional-DETR | 56.470 | 35.552 |

### A100 (ุญุฌู ุงูุฏูุนุฉ: 16)

| ุงููููุฉ/ุงููููุฐุฌ | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุจุฏูู ุชุฌููุน | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุชุฌููุน |
|:---:|:---:|:---:|
| ุชุตููู ุงูุตูุฑ/ViT | 40.944 | 40.010 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Segformer | 37.005 | 31.144 |
| ุชุตููู ุงูุตูุฑ/BeiT | 41.854 | 41.048 | 
| ุงูุชุดุงู ุงููุงุฆูุงุช/DETR | 164.382 | 161.902 |
| ุชุตููู ุงูุตูุฑ/ConvNeXT | 82.258 | 75.561 | 
| ุชุตููู ุงูุตูุฑ/ResNet | 7.018 | 5.024 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Mask2former | 178.945 | 154.814 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Maskformer | 638.570 | 579.826 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/MobileNet | 51.693 | 30.310 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Resnet-101 | 232.887 | 155.021 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Conditional-DETR | 180.491 | 124.032 |

### V100 (ุญุฌู ุงูุฏูุนุฉ: 1)

| ุงููููุฉ/ุงููููุฐุฌ | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุจุฏูู ุชุฌููุน | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุชุฌููุน |
|:---:|:---:|:---:|
| ุชุตููู ุงูุตูุฑ/ViT | 10.495 | 6.00 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Segformer | 13.321 | 5.862 | 
| ุงูุชุดุงู ุงููุงุฆูุงุช/OwlViT | 25.769 | 22.395 | 
| ุชุตููู ุงูุตูุฑ/BeiT | 11.347 | 7.234 | 
| ุงูุชุดุงู ุงููุงุฆูุงุช/DETR | 33.951 | 19.388 |
| ุชุตููู ุงูุตูุฑ/ConvNeXT | 11.623 | 10.412 | 
| ุชุตููู ุงูุตูุฑ/ResNet | 6.484 | 3.820 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Mask2former | 64.640 | 49.873 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Maskformer | 95.532 | 72.207 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/MobileNet | 9.217 | 4.753 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Resnet-101 | 52.818 | 28.367 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Conditional-DETR | 39.512 | 20.816 |

### V100 (ุญุฌู ุงูุฏูุนุฉ: 4)

| ุงููููุฉ/ุงููููุฐุฌ | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุจุฏูู ุชุฌููุน | ุงูุฅุตุฏุงุฑ 2.0 ูู torch - <br>ุชุฌููุน |
|:---:|:---:|:---:|
| ุชุตููู ุงูุตูุฑ/ViT | 15.181 | 14.501 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Segformer | 16.787 | 16.188 |
| ุชุตููู ุงูุตูุฑ/BeiT | 15.171 | 14.753 | 
| ุงูุชุดุงู ุงููุงุฆูุงุช/DETR | 88.529 | 64.195 |
| ุชุตููู ุงูุตูุฑ/ConvNeXT | 29.574 | 27.085 | 
| ุชุตููู ุงูุตูุฑ/ResNet | 6.109 | 4.731 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Mask2former | 90.402 | 76.926 |
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/Maskformer | 234.261 | 205.456 | 
| ุชุฌุฒุฆุฉ ุงูุตูุฑุฉ/MobileNet | 24.623 | 14.816 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Resnet-101 | 134.672 | 101.304 |
| ุงูุชุดุงู ุงููุงุฆูุงุช/Conditional-DETR | 97.464 | 69.739 |

### T4 (batch size: 16)

| **Task/Model** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|
| Image Classification/ViT | 163.914 | 160.907 | 
| Image Segmentation/Segformer | 192.412 | 163.620 |
| Image Classification/BeiT | 188.978 | 187.976 | 
| Object Detection/DETR | OOM | OOM |
| Image Classification/ConvNeXT | 422.886 | 388.078 | 
| Image Classification/ResNet | 44.114 | 37.604 |
| Image Segmentation/Mask2former | 756.337 | 695.291 |
| Image Segmentation/Maskformer | 2842.940 | 2656.88 | 
| Image Segmentation/MobileNet | 299.003 | 201.942 |
| Object Detection/Resnet-101 |  1619.505 | 1262.758 | 
| Object Detection/Conditional-DETR | 1137.513 | 897.390|

## PyTorch Nightly
We also benchmarked on PyTorch nightly (2.1.0dev, find the wheel [here](https://download.pytorch.org/whl/nightly/cu118)) and observed improvement in latency both for uncompiled and compiled models. 

### A100

| **Task/Model** | **Batch Size** | **torch 2.0 - no compile** | **torch 2.0 -<br> compile** |
|:---:|:---:|:---:|:---:|
| Image Classification/BeiT | Unbatched | 12.462 | 6.954 | 
| Image Classification/BeiT | 4 | 14.109 | 12.851 | 
| Image Classification/BeiT | 16 | 42.179 | 42.147 | 
| Object Detection/DETR | Unbatched | 30.484 | 15.221 |
| Object Detection/DETR | 4 | 46.816 | 30.942 |
| Object Detection/DETR | 16 | 163.749 | 163.706  |

### T4

| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|:---:|
| Image Classification/BeiT | Unbatched | 14.408 | 14.052 | 
| Image Classification/BeiT | 4 | 47.381 | 46.604 | 
| Image Classification/BeiT | 16 | 42.179 | 42.147  | 
| Object Detection/DETR | Unbatched | 68.382 | 53.481 |
| Object Detection/DETR | 4 | 269.615 | 204.785 |
| Object Detection/DETR | 16 | OOM | OOM   |

### V100

| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|:---:|
| Image Classification/BeiT | Unbatched | 13.477 | 7.926 | 
| Image Classification/BeiT | 4 | 15.103 | 14.378 | 
| Image Classification/BeiT | 16 | 52.517 | 51.691  | 
| Object Detection/DETR | Unbatched | 28.706 | 19.077 |
| Object Detection/DETR | 4 | 88.402 | 62.949|
| Object Detection/DETR | 16 | OOM | OOM  |


## Reduce Overhead
We benchmarked `reduce-overhead` compilation mode for A100 and T4 in Nightly.

### A100

| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** |
|:---:|:---:|:---:|:---:|
| Image Classification/ConvNeXT | Unbatched | 11.758 | 7.335 | 
| Image Classification/ConvNeXT | 4 | 23.171 | 21.490 | 
| Image Classification/ResNet | Unbatched | 7.435 | 3.801 | 
| Image Classification/ResNet | 4 | 7.261 | 2.187 | 
| Object Detection/Conditional-DETR | Unbatched | 32.823 | 11.627  | 
| Object Detection/Conditional-DETR | 4 | 50.622 | 33.831  | 
| Image Segmentation/MobileNet | Unbatched | 9.869 | 4.244 |
| Image Segmentation/MobileNet | 4 | 14.385 | 7.946 |


### T4

| **Task/Model** | **Batch Size** | **torch 2.0 - <br>no compile** | **torch 2.0 - <br>compile** | 
|:---:|:---:|:---:|:---:|
| Image Classification/ConvNeXT | Unbatched | 32.137 | 31.84 | 
| Image Classification/ConvNeXT | 4 | 120.944 | 110.209 | 
| Image Classification/ResNet | Unbatched | 9.761 | 7.698 | 
| Image Classification/ResNet | 4 | 15.215 | 13.871 | 
| Object Detection/Conditional-DETR | Unbatched | 72.150 | 57.660  | 
| Object Detection/Conditional-DETR | 4 | 301.494 | 247.543  | 
| Image Segmentation/MobileNet | Unbatched | 22.266 | 19.339  |
| Image Segmentation/MobileNet | 4 | 78.311 | 50.983 |